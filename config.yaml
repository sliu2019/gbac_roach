
seed: 0

experiment_type: 'terrain_types'                      # Name of the task

# If testing, this is the model to load. If training and curr_agg_iter != 0 (in other words, we're aggregating), then this is the model to do additional training on top of. 

previous_dynamics_model: "/home/anagabandi/rllab-private/data/local/experiment/MAML_roach/9_11_optimization/_ubs_23_ulr_0.0num_updates1_layers_2_x500_task_list_turf_styrofoam_carpet_mlr_0.001_mbs_64_num-sgd-steps_1_reg_weight_0.001_dim_bias_5_metatrain_lr_False/model_aggIter0_epoch45"


training:
    meta_batch_size: 250                   # Number of tasks sampled per meta-update
    update_batch_size: 16                # Number of data points per inner-update ("K" in k-shot learning)
    meta_lr: 0.001                       # Meta learning rate
    update_lr: 2.0                      # Inner-update learning rate
    num_updates: 3                       # Number of inner updates
    num_sgd_steps: 2                    # The number of gradient steps to take in a single "inner update"; the steps are taken on a sliding window of the training data
    meta_loss: False                     # If True, learns the loss in the inner-update
    steps: 0.001
    training_ratio: 0.9                  # Training/validation split
    use_reg: True                        # If True, regularize the outer loss function
    regularization_weight: 0.001         # 0.001 # Weight on regularization term in loss function
    use_clf: True                         # Temporary: true means clear figure after plotting each time 
    use_clip: True                      # Temporary: true means clip the gradients
    max_runs_per_surface: 396
    task_list:
        - "all"
    weight_initializer: "xavier"        # "xavier" or "truncated_normal"
    optimizer: "adam"                   # adam, momentum, adagrad, etc. 
    use_momentum: True                  # If we're using momentum or not
    momentum: 0.9                       # The momentum parameter for momentum optimizer, usually 0.9
    learn_inner_loss: False

testing:
    animated: False                       # If True, renders the test rollouts
    file: '/home/ignasi/data_lta/CRIPPLE/MAML/HalfCheetahEnv__cripple__multi_updates_1__meta_batch_size_32__meta_learn_lr_False__update_batch_size_32__batch_size_2000__num_updates_1__multi_input_0__update_lr_0.001__dim_bias_0__seed_605__metatrain_itr_10/'
    update_batch_size: 16                 # Number of data points per inner-update ("K" in k-shot learning)
    meta_lr: 0.001
    update_lr: 2.0
    num_updates: 3                       # Number of inner updates
    num_sgd_steps: 2    
    
    max_path_length: 50                 # Max length of rollout
    num_rollouts: 1                    # Number of rollouts to perform for testing
    speedup: 1                           # Factor by which to speed-up the rendering of the rollouts
    dynamic_evaluation: False
    render:
        stationary_camera: False
        xpos: 0
        ypos: 0
        width: 1000
        height: 1000
        title: 'Test'

aggregation:
    curr_agg_iter: 0                    # which aggregation iteration you're on (model0 results in runs0, then on iter 1 you load in runs0 to train model1 and then execute it to get runs1)
    #agg_datapaths:                      # List of saved_rollout folders from on-policy runs
    #    -"/home/anagabandi/rllab-private/data/local/experiment/MAML_roach_copy/Wednesday_optimization/ulr_5_num_update_1/_ubs_8_ulr_2.0num_updates1_layers_1_x100_task_list_all/carpet/saved_rollouts"
    ratio_new: 0.9                      # The ratio of on pol to off pol. If None, that means a straightforward concatenation of off and on pol

roach:
    frequency_value: 10
    serial_port: '/dev/ttyUSB0'
    baud_rate: 57600
    visualize_rviz: True
    x_index: 0
    y_index: 1
    yaw_cos_index: 10
    yaw_sin_index: 11

model:
    norm: None                          # Norm options: [None, 'batch_norm', 'layer_norm']
    dim_hidden:                         # Dimension of the hidden layers
        - 23
        - 23
    dim_conv1d:                         # Dimension of the 1d convolutions when learning the inner loss
        - 8
        - 8
        - 8
    dim_bias: 5
    nonlinearity: "relu"                      # All lower case: can be "relu", "tanh" 


sampler:     
    batch_size: 200                     # Total number of datapoints collected during each data collection session (PER task)
        #5 rollouts per each of 4 surfaces = 20 rollouts = 1k pts ... make batchsize 200
    max_path_length: 50                 # Maximum path length
    max_epochs: 2                      # Maximum number of epochs before collecting new data
    n_itr_rand: 1                       # Number of times we collect random data (instead of on-policy data)
    max_buffer: 10000000                # Maximum number of timesteps in our buffer (used for retraining dynamics model)
    train_policy: False
    n_itr_policy: 5 ##????????
    multi_input: 0

logging:
    log: True                           # Enables/Disables all the logging options
    resume: False                       # Resume the training
    train: True                         # Train vs. testing
    test_itr: -1                        # Iteration to load model (-1 for the latest model)
    save_itr: 10                        # Number of gradient updates between saving the model
    print_itr: 10                       # Number of gradient updates between printing results
    summary_itr: 10                     # Number of gradient updates between summarizing results
    log_dir: 'MAML_roach/'              # Directory where we save the MAML data
    resume_dir: ''

policy:
    n_candidates: 2000                   # Number of candidates actions when performing MPC
    horizon: 5                          # Horizon of the planning in the MPC
    test_regressor: False
    horiz_penalty_factor: 50
    backward_discouragement: 10
    heading_penalty_factor: 5      #40,10,10 zigzag, everything else is 50,10,5 ################# 50, 5, 15 for front r leg missing
